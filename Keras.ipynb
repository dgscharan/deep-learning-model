{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Keras \n\n creating a Keras model by loading a data set, preprocessing input data, building a Sequential Keras model and compiling the model with a training configuration. Afterwards training the model on the training data and evaluate it on the test set\n\n##\u00a0Data\n\n the Reuters newswire dataset is used . This dataset consists of 11,228 newswires from the Reuters news agency. Each wire is encoded as a sequence of word indexes, This dataset is available through the Keras API.\n\n## Goal\n to create a Multi-layer perceptron (MLP) using Keras which we can train to classify news items into the specified 46 topics.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "source": "import pip\n\ntry:\n    __import__('keras')\nexcept ImportError:\n    pip.main(['install', 'keras']) \n    \ntry:\n    __import__('h5py')\nexcept ImportError:\n    pip.main(['install', 'h5py']) \n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\n\nseed = 1337\nnp.random.seed(seed)"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.datasets import reuters\n\nmax_words = 1000\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2,\n                                                         seed=seed)\nnum_classes = np.max(y_train) + 1  # 46 topics"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
        }, 
        {
            "source": "# label encoding\n\nUsed to_categorical, to transform both *y_train* and *y_test* into one-hot encoded vectors of length *num_classes*:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y_train = to_categorical(y_train , num_classes)\ny_test = to_categorical(y_test , num_classes)"
        }, 
        {
            "source": "# model definition\n\n initialising a Keras *Sequential* model and add three layers to it:\n\n    Layer: Adding a *Dense* layer with in input_shape=(max_words,), 512 output units and \"relu\" activation.\n    Layer: Adding a *Dropout* layer with dropout rate of 50%.\n    Layer: Adding a *Dense* layer with num_classes output units and \"softmax\" activation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model = Sequential()\nmodel.add(Dense(512,activation = 'relu' , input_shape =(max_words,)))\nmodel.add(Dropout(.50)) \nmodel.add(Dense(num_classes , activation = 'softmax')) "
        }, 
        {
            "source": "# model compilation\n\n we need to compile our Keras model with a training configuration. Compile your model with \"categorical_crossentropy\" as loss function, \"adam\" as optimizer and specify \"accuracy\" as evaluation metric. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam'  ,  metrics=['accuracy'] )"
        }, 
        {
            "source": " #model training and evaluation\n\ndefining  the batch_size for training as 32 and train the model for 5 epochs on *x_train* and *y_train* by using the *fit* method of the  model and  the score is calculated for trained model by running *evaluate* on *x_test* and *y_test* with the same batch size as used in *fit*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Train on 8982 samples, validate on 2246 samples\nEpoch 1/5\n8982/8982 [==============================] - 30s 3ms/step - loss: 1.3943 - acc: 0.6902 - val_loss: 0.9758 - val_acc: 0.7738\nEpoch 2/5\n8982/8982 [==============================] - 29s 3ms/step - loss: 0.7741 - acc: 0.8186 - val_loss: 0.8232 - val_acc: 0.8063\nEpoch 3/5\n8982/8982 [==============================] - 34s 4ms/step - loss: 0.5572 - acc: 0.8653 - val_loss: 0.8173 - val_acc: 0.8117\nEpoch 4/5\n8982/8982 [==============================] - 30s 3ms/step - loss: 0.4228 - acc: 0.8935 - val_loss: 0.8281 - val_acc: 0.8032\nEpoch 5/5\n8982/8982 [==============================] - 29s 3ms/step - loss: 0.3478 - acc: 0.9096 - val_loss: 0.8575 - val_acc: 0.7992\n2246/2246 [==============================] - 1s 492us/step\n"
                }
            ], 
            "source": "batch_size=32\nmodel.fit(x_train , y_train, \n          batch_size=batch_size,\n          epochs = 5,\n          validation_data = (x_test , y_test))\nscore = model.evaluate(x_test , y_test , batch_size=batch_size)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.79919857524487981"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "score[1]"
        }, 
        {
            "source": "saving model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.save(\"model.h5\")  "
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!base64 model.h5 > model.h5.base64"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}